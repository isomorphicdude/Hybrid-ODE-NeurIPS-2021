{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable autorreload of modules\n",
    "%load_ext autoreload\n",
    "import os\n",
    "# os.setpriority(\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from new_models import *\n",
    "from SDE_helper import *\n",
    "import warnings\n",
    "# warnings.filterwarnings('always', module='.*')\n",
    "import logging\n",
    "# logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initial Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_level = 1.0\n",
    "# dg = pickle.load(open(\"data/datafile_dose_exp_test.pkl\", \"rb\"))\n",
    "# seed = 666\n",
    "# torch.manual_seed(seed)\n",
    "# noise = torch.randn_like(dg.measurements) * (noise_level - 0.2)\n",
    "# with torch.no_grad():\n",
    "#     dg.measurements = dg.measurements + noise\n",
    "#     dg.split_sample()\n",
    "\n",
    "# with open(\"data/datafile_dose_noise_{}.pkl\".format(noise_level), \"wb\") as f:\n",
    "#     pickle.dump(dg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ablation study: True\n",
      "Conditioning on w only: False\n",
      "Number of parameters of y net: 22\n",
      "Conditioning on expert ODEs: True\n",
      "Conditioning expert ODEs only False\n"
     ]
    }
   ],
   "source": [
    "from new_models import SDENet, NewEncoderLSTM, NewVariationalInference\n",
    "with open('data/datafile_dim8.pkl', \"rb\") as f:\n",
    "# with open('data/datafile_dose_noise_1.0.pkl', \"rb\") as f:\n",
    "    data_gen = pickle.load(f)\n",
    "    \n",
    "data_config = sim_config.dim8_config\n",
    "# data_config = sim_config.DataConfig(n_sample=1000)\n",
    "\n",
    "obs_dim = data_config.obs_dim\n",
    "latent_dim = data_config.latent_dim\n",
    "action_dim = data_config.action_dim\n",
    "t_max = data_config.t_max\n",
    "step_size = data_config.step_size\n",
    "encoder_latent_ratio = 2.0\n",
    "encoder_output_dim = 6\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'mps'\n",
    "normalize = True\n",
    "prior = ExponentialPrior.log_density\n",
    "ablate = True\n",
    "condition_w = True\n",
    "condition_w_only = False\n",
    "\n",
    "\n",
    "\n",
    "encoder = NewEncoderLSTM(\n",
    "            obs_dim + action_dim,\n",
    "            int(obs_dim * encoder_latent_ratio),\n",
    "            encoder_output_dim,\n",
    "            device=device,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "\n",
    "\n",
    "new_decoder = SDENet(\n",
    "    input_size=(encoder_output_dim,),\n",
    "    device=device,\n",
    "    condition_w=condition_w,\n",
    "    obs_dim=obs_dim,\n",
    "    y_net_hidden_width=4,\n",
    "    sigma=0.01,\n",
    "    dt=0.001,\n",
    "    weight_network_sizes=(32,), #128\n",
    "    control_network_width=4, #8\n",
    "    ablate=ablate,\n",
    "    condition_w_only=condition_w_only,\n",
    ")\n",
    "\n",
    "\n",
    "new_vi = NewVariationalInference(encoder, \n",
    "                            new_decoder, \n",
    "                            prior_log_pdf=prior, \n",
    "                            elbo=True,\n",
    "                            kl_coeff1=0.2,\n",
    "                            kl_coeff2=0.2,\n",
    "                            use_full_mc_kl=False,\n",
    "                            use_full_mc_lik=False,\n",
    "                            lik_mc_size=10,\n",
    "                            mc_size=100,\n",
    "                            )\n",
    "\n",
    "if not condition_w:\n",
    "    new_params = (\n",
    "                list(new_vi.encoder.parameters())\n",
    "                + list(new_vi.decoder.y_net.parameters())\n",
    "                + list(new_vi.decoder.w_net.parameters())\n",
    "                + list(new_vi.decoder.projection.parameters())\n",
    "            )\n",
    "else:\n",
    "    new_params = (\n",
    "                list(new_vi.encoder.parameters()) +\n",
    "                list(new_vi.decoder.y_net.parameters())\n",
    "                + list(new_vi.decoder.w_net.parameters())\n",
    "                + list(new_vi.decoder.projection.parameters())\n",
    "                + list(new_vi.decoder.control_net.parameters())\n",
    "            \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 22:17:53,074 Training VI_LSTMEncoder_HybridDecoder.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size -100\n",
      "n_sample 1000\n",
      "iter 1\n",
      "iter 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected value argument (Tensor of shape (64, 6)) to be within the support (GreaterThanEq(lower_bound=0.0)) of the distribution Exponential(rate: tensor([100.])), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan]], grad_fn=<IndexPutBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dg\u001b[39m.\u001b[39mset_device(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m dg\u001b[39m.\u001b[39mset_train_size(\u001b[39m1000\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m variational_training_loop(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m1000\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     data_gen,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     new_vi,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m64\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     torch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdam(new_params, lr\u001b[39m=\u001b[39;49m\u001b[39m1e-6\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     best_on_disk\u001b[39m=\u001b[39;49m\u001b[39m1e9\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     early_stop\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mnew-models/\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     print_future_mse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isomorphicdude/MSc_nosync/Hybrid-ODE-NeurIPS-2021/ro.ipynb#X44sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n",
      "File \u001b[0;32m~/MSc_nosync/Hybrid-ODE-NeurIPS-2021/training_utils.py:62\u001b[0m, in \u001b[0;36mvariational_training_loop\u001b[0;34m(niters, data_generator, model, batch_size, optimizer, test_freq, best_on_disk, early_stop, path, shuffle, train_fold, new, print_future_mse, t0, hybrid_cde)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39m# print('compute loss')\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m# loss = model.loss(data)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m# print(model.decoder.flat_initial_params)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m# print(list(model.decoder.projection.parameters()))\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39miter \u001b[39m\u001b[39m{\u001b[39;00mitr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mloss(data)\n\u001b[1;32m     63\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     64\u001b[0m \u001b[39m# print('calculate backward')\u001b[39;00m\n",
      "File \u001b[0;32m~/MSc_nosync/Hybrid-ODE-NeurIPS-2021/new_models.py:646\u001b[0m, in \u001b[0;36mNewVariationalInference.loss\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNot implemented yet for analytic KL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    643\u001b[0m     \u001b[39m# kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim=1), dim=0)\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    645\u001b[0m     \u001b[39m# monte carlo KL over batch\u001b[39;00m\n\u001b[0;32m--> 646\u001b[0m     kld_loss_vae \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmc_kl_vae(mu, log_var), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    648\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_full_mc_kl:\n\u001b[1;32m    649\u001b[0m         kld_loss_sde \u001b[39m=\u001b[39m kl_of_w\n",
      "File \u001b[0;32m~/MSc_nosync/Hybrid-ODE-NeurIPS-2021/new_models.py:671\u001b[0m, in \u001b[0;36mNewVariationalInference.mc_kl_vae\u001b[0;34m(self, mu, log_var)\u001b[0m\n\u001b[1;32m    669\u001b[0m z[z \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon\n\u001b[1;32m    670\u001b[0m \u001b[39m# log p(z)\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m log_p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprior_log_pdf(z)\n\u001b[1;32m    672\u001b[0m \u001b[39m# log q(z)\u001b[39;00m\n\u001b[1;32m    673\u001b[0m log_q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mlog_density(mu, log_var, z)\n",
      "File \u001b[0;32m~/MSc_nosync/Hybrid-ODE-NeurIPS-2021/new_models.py:46\u001b[0m, in \u001b[0;36mExponentialPrior.log_density\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_density\u001b[39m(z):\n\u001b[1;32m     45\u001b[0m     n \u001b[39m=\u001b[39m dist\u001b[39m.\u001b[39mexponential\u001b[39m.\u001b[39mExponential(rate\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor([\u001b[39m100.0\u001b[39m])\u001b[39m.\u001b[39mto(z))\n\u001b[0;32m---> 46\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msum(n\u001b[39m.\u001b[39;49mlog_prob(z), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda/envs/jax/lib/python3.9/site-packages/torch/distributions/exponential.py:64\u001b[0m, in \u001b[0;36mExponential.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[1;32m     63\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_args:\n\u001b[0;32m---> 64\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_sample(value)\n\u001b[1;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrate\u001b[39m.\u001b[39mlog() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrate \u001b[39m*\u001b[39m value\n",
      "File \u001b[0;32m~/miniconda/envs/jax/lib/python3.9/site-packages/torch/distributions/distribution.py:300\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    298\u001b[0m valid \u001b[39m=\u001b[39m support\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    301\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected value argument \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    303\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto be within the support (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(support)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof the distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected value argument (Tensor of shape (64, 6)) to be within the support (GreaterThanEq(lower_bound=0.0)) of the distribution Exponential(rate: tensor([100.])), but found invalid values:\ntensor([[nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan, nan]], grad_fn=<IndexPutBackward0>)"
     ]
    }
   ],
   "source": [
    "from training_utils import *\n",
    "with open('data/datafile_dim8.pkl', \"rb\") as f:\n",
    "    data_gen = pickle.load(f)\n",
    "dg = data_gen\n",
    "dg.set_device(device)\n",
    "dg.set_train_size(1000)\n",
    "variational_training_loop(\n",
    "    1000,\n",
    "    data_gen,\n",
    "    new_vi,\n",
    "    64,\n",
    "    torch.optim.Adam(new_params, lr=1e-6),\n",
    "    10,\n",
    "    best_on_disk=1e9,\n",
    "    early_stop=100,\n",
    "    path='new-models/',\n",
    "    print_future_mse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Second Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from new_models import SDENet, NewEncoderLSTM, NewVariationalInference, SDENet2\n",
    "# with open('data/datafile_dim8.pkl', \"rb\") as f:\n",
    "# # with open('data/datafile_dose_noise_1.0.pkl', \"rb\") as f:\n",
    "#     data_gen = pickle.load(f)\n",
    "    \n",
    "# data_config = sim_config.dim8_config\n",
    "# # data_config = sim_config.DataConfig(n_sample=1000)\n",
    "\n",
    "# obs_dim = data_config.obs_dim\n",
    "# latent_dim = data_config.latent_dim\n",
    "# action_dim = data_config.action_dim\n",
    "# t_max = data_config.t_max\n",
    "# step_size = data_config.step_size\n",
    "# encoder_latent_ratio = 2.0\n",
    "# encoder_output_dim = 6\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # device = 'mps'\n",
    "# normalize = True\n",
    "# prior = ExponentialPrior.log_density\n",
    "# ablate = True\n",
    "# condition_w = False\n",
    "# condition_w_only = False\n",
    "\n",
    "\n",
    "\n",
    "# encoder = NewEncoderLSTM(\n",
    "#             obs_dim + action_dim,\n",
    "#             int(obs_dim * encoder_latent_ratio),\n",
    "#             encoder_output_dim,\n",
    "#             device=device,\n",
    "#             normalize=normalize,\n",
    "#         )\n",
    "\n",
    "\n",
    "# new_decoder = SDENet2(\n",
    "#     input_size=(encoder_output_dim,),\n",
    "#     device=device,\n",
    "#     obs_dim=obs_dim,\n",
    "#     y_net_hidden_width=None,\n",
    "#     sigma=0.1,\n",
    "#     weight_network_sizes=(1,16, 1), #128\n",
    "# )\n",
    "\n",
    "\n",
    "# new_vi = NewVariationalInference(encoder, \n",
    "#                             new_decoder, \n",
    "#                             prior_log_pdf=prior, \n",
    "#                             elbo=True,\n",
    "#                             kl_coeff1=0.2,\n",
    "#                             kl_coeff2=0.2,\n",
    "#                             use_full_mc_kl=False,\n",
    "#                             use_full_mc_lik=False,\n",
    "#                             lik_mc_size=10,\n",
    "#                             mc_size=100,\n",
    "#                             )\n",
    "\n",
    "# new_params = (\n",
    "#             list(new_vi.encoder.parameters())\n",
    "#             + list(new_vi.decoder.y_net.parameters())\n",
    "#             + list(new_vi.decoder.w_net.parameters())\n",
    "#             + list(new_vi.decoder.projection.parameters())\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from training_utils import *\n",
    "# with open('data/datafile_dim8.pkl', \"rb\") as f:\n",
    "#     data_gen = pickle.load(f)\n",
    "# dg = data_gen\n",
    "# dg.set_device(device)\n",
    "# dg.set_train_size(1000)\n",
    "# variational_training_loop(\n",
    "#     1000,\n",
    "#     data_gen,\n",
    "#     new_vi,\n",
    "#     64,\n",
    "#     torch.optim.Adam(new_params, lr=1e-2),\n",
    "#     10,\n",
    "#     best_on_disk=1e9,\n",
    "#     early_stop=100,\n",
    "#     path='new-models/',\n",
    "#     print_future_mse=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from training_utils import evaluate\n",
    "# evaluate(new_vi, \n",
    "#          data_gen, \n",
    "#          64, \n",
    "#          5,\n",
    "#          mc_itr=50, \n",
    "#          real=False)\n",
    "\n",
    "# # dim fw=(1,64,1), without weights constraint\n",
    "# # (0.25526687502861023,\n",
    "# #  0.0013087330489873566,\n",
    "# #  0.06703939384492985,\n",
    "# #  0.9209802746772766,\n",
    "# #  0.006640289111317933,\n",
    "# #  0.668211308672912)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Another attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new_models import SDENet3, NewEncoderLSTM, NewVariationalInference\n",
    "with open('data/datafile_dim8.pkl', \"rb\") as f:\n",
    "# with open('data/datafile_dose_noise_1.0.pkl', \"rb\") as f:\n",
    "    data_gen = pickle.load(f)\n",
    "    \n",
    "data_config = sim_config.dim8_config\n",
    "# data_config = sim_config.DataConfig(n_sample=1000)\n",
    "\n",
    "obs_dim = data_config.obs_dim\n",
    "latent_dim = data_config.latent_dim\n",
    "action_dim = data_config.action_dim\n",
    "t_max = data_config.t_max\n",
    "step_size = data_config.step_size\n",
    "encoder_latent_ratio = 2.0\n",
    "encoder_output_dim = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'mps'\n",
    "normalize = True\n",
    "prior = ExponentialPrior.log_density\n",
    "ablate = False\n",
    "condition_w = False\n",
    "condition_w_only = False\n",
    "\n",
    "\n",
    "\n",
    "encoder = NewEncoderLSTM(\n",
    "            obs_dim + action_dim,\n",
    "            int(obs_dim * encoder_latent_ratio),\n",
    "            encoder_output_dim,\n",
    "            device=device,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "\n",
    "\n",
    "new_decoder = SDENet3(\n",
    "    input_size=(encoder_output_dim,),\n",
    "    device=device,\n",
    "    obs_dim=obs_dim,\n",
    "    action_dim=action_dim,\n",
    "    latent_dim=encoder_output_dim,\n",
    "    y_net_hidden_width=None,\n",
    "    batch_size=64,\n",
    "    sigma=0.01,\n",
    "    weight_network_sizes=(1,), #128\n",
    ")\n",
    "\n",
    "\n",
    "new_vi = NewVariationalInference(encoder, \n",
    "                            new_decoder, \n",
    "                            prior_log_pdf=prior, \n",
    "                            elbo=True,\n",
    "                            kl_coeff1=0.2,\n",
    "                            kl_coeff2=0.2,\n",
    "                            use_full_mc_kl=False,\n",
    "                            use_full_mc_lik=False,\n",
    "                            lik_mc_size=10,\n",
    "                            mc_size=100,\n",
    "                            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_params = (\n",
    "            list(new_vi.encoder.parameters())\n",
    "            + list(new_vi.decoder.y_net.parameters())\n",
    "            + list(new_vi.decoder.controlled_w.parameters())\n",
    "            + list(new_vi.decoder.projection.parameters())\n",
    ")\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_utils import *\n",
    "with open('data/datafile_dim8.pkl', \"rb\") as f:\n",
    "    data_gen = pickle.load(f)\n",
    "dg = data_gen\n",
    "dg.set_device(device)\n",
    "dg.set_train_size(1000)\n",
    "variational_training_loop(\n",
    "    1000,\n",
    "    dg,\n",
    "    new_vi,\n",
    "    64,\n",
    "    torch.optim.Adam(new_params, lr=1e-3),\n",
    "    10,\n",
    "    best_on_disk=1e9,\n",
    "    early_stop=100,\n",
    "    path='new-models/',\n",
    "    print_future_mse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from training_utils import evaluate\n",
    "# evaluate(new_vi, \n",
    "#          data_gen, \n",
    "#          64, \n",
    "#          5,\n",
    "#          mc_itr=50, \n",
    "#          real=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Another attempt\n",
    "\n",
    "## Train expert model for latent dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from training_utils import *\n",
    "\n",
    "with open('data/datafile_dim8.pkl', \"rb\") as f:\n",
    "# with open('data/datafile_dose_noise_1.0.pkl', \"rb\") as f:\n",
    "    data_gen = pickle.load(f)\n",
    "    \n",
    "data_config = sim_config.dim8_config\n",
    "# data_config = sim_config.DataConfig(n_sample=1000)\n",
    "\n",
    "obs_dim = data_config.obs_dim\n",
    "latent_dim = data_config.latent_dim\n",
    "action_dim = data_config.action_dim\n",
    "t_max = data_config.t_max\n",
    "step_size = data_config.step_size\n",
    "encoder_latent_ratio = 2.0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "normalize = True\n",
    "prior = ExponentialPrior.log_density\n",
    "encoder_output_dim = 4\n",
    "\n",
    "old_encoder = EncoderLSTM(\n",
    "            obs_dim + action_dim,\n",
    "            int(obs_dim * encoder_latent_ratio),\n",
    "            4,\n",
    "            device=device,\n",
    "            normalize=normalize,\n",
    "        )\n",
    "\n",
    "old_decoder = RocheExpertDecoder(\n",
    "            obs_dim,\n",
    "            4,\n",
    "            action_dim,\n",
    "            14,\n",
    "            1,\n",
    "            roche=True,\n",
    "            method=\"dopri5\",\n",
    "            device=device,\n",
    "            ablate=False,\n",
    "        )\n",
    "old_vi = VariationalInference(old_encoder,\n",
    "                                old_decoder,\n",
    "                                prior_log_pdf=prior,\n",
    "                                elbo=True)\n",
    "\n",
    "params = (\n",
    "            list(old_vi.encoder.parameters())\n",
    "            + list(old_vi.decoder.output_function.parameters())\n",
    "            + list(old_vi.decoder.ode.ml_net.parameters())\n",
    "        )\n",
    "\n",
    "variational_training_loop(\n",
    "    1,\n",
    "    data_gen,\n",
    "    old_vi,\n",
    "    50,\n",
    "    torch.optim.Adam(params, lr=1e-2),\n",
    "    5,\n",
    "    early_stop=10,\n",
    "    print_future_mse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(old_vi,\n",
    "#             data_gen,\n",
    "#             50,\n",
    "#             5,\n",
    "#             mc_itr=50,\n",
    "#             real=False)\n",
    "# (0.026531780138611794,\n",
    "#  0.0005103847261433787,\n",
    "#  0.008222115392954869,\n",
    "#  0.7614899277687073,\n",
    "#  0.005279327186749542,\n",
    "#  0.5488908503766188)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train latent dynamics model as neural controlled ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from new_models import NewEncoderLSTM\n",
    "# from new_models_hybrid import SDENet4, HybridVariationalInference\n",
    "\n",
    "# with open('data/datafile_dim8.pkl', \"rb\") as f:\n",
    "# # with open('data/datafile_dose_noise_1.0.pkl', \"rb\") as f:\n",
    "#     data_gen = pickle.load(f)\n",
    "    \n",
    "# data_config = sim_config.dim8_config\n",
    "# # data_config = sim_config.DataConfig(n_sample=1000)\n",
    "\n",
    "# obs_dim = data_config.obs_dim\n",
    "# latent_dim = data_config.latent_dim\n",
    "# action_dim = data_config.action_dim\n",
    "# t_max = data_config.t_max\n",
    "# step_size = data_config.step_size\n",
    "# encoder_latent_ratio = 2.0\n",
    "# encoder_output_dim = 6\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # device = 'mps'\n",
    "# normalize = True\n",
    "# prior = ExponentialPrior.log_density\n",
    "# ablate = False\n",
    "\n",
    "\n",
    "# from model import EncoderLSTM\n",
    "\n",
    "# old_encoder = EncoderLSTM(\n",
    "#             obs_dim + action_dim,\n",
    "#             int(obs_dim * encoder_latent_ratio),\n",
    "#             4,\n",
    "#             device=device,\n",
    "#             normalize=normalize,\n",
    "#         )\n",
    "# model_path = 'model/VI_LSTMEncoder_ExpertDecoder.pkl'\n",
    "# old_vi = torch.load(model_path)\n",
    "# old_encoder.load_state_dict(torch.load(model_path)['encoder_state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "# encoder = NewEncoderLSTM(\n",
    "#             obs_dim + action_dim,\n",
    "#             int(obs_dim * encoder_latent_ratio),\n",
    "#             encoder_output_dim,\n",
    "#             device=device,\n",
    "#             normalize=normalize,\n",
    "#         )\n",
    "\n",
    "\n",
    "# new_decoder = SDENet4(\n",
    "#     input_size=(encoder_output_dim,),\n",
    "#     device=device,\n",
    "#     obs_dim=obs_dim,\n",
    "#     action_dim=action_dim,\n",
    "#     latent_dim=encoder_output_dim,\n",
    "#     y_net_hidden_size=(1,),\n",
    "#     augmented_dim=4,\n",
    "#     batch_size=64,\n",
    "#     sigma=0.01,\n",
    "#     weight_network_sizes=(1,8,1), #128\n",
    "# )\n",
    "\n",
    "\n",
    "# new_vi = HybridVariationalInference(encoder, \n",
    "#                             new_decoder, \n",
    "#                             prior_log_pdf=prior, \n",
    "#                             elbo=True,\n",
    "#                             kl_coeff1=0.2,\n",
    "#                             kl_coeff2=0.2,\n",
    "#                             use_full_mc_kl=False,\n",
    "#                             use_full_mc_lik=False,\n",
    "#                             lik_mc_size=10,\n",
    "#                             mc_size=10,\n",
    "#                             expert_encoder=old_encoder,\n",
    "#                             )\n",
    "\n",
    "\n",
    "# new_params = (\n",
    "#             list(new_vi.encoder.parameters())\n",
    "#             + list(new_vi.decoder.y_net.parameters())\n",
    "#             + list(new_vi.decoder.w_net.parameters())\n",
    "#             + list(new_vi.decoder.projection.parameters())\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from training_utils import *\n",
    "# with open('data/datafile_dim8.pkl', \"rb\") as f:\n",
    "#     data_gen = pickle.load(f)\n",
    "# dg = data_gen\n",
    "# dg.set_device(device)\n",
    "# dg.set_train_size(1000)\n",
    "# variational_training_loop(\n",
    "#     5000,\n",
    "#     data_gen,\n",
    "#     new_vi,\n",
    "#     64,\n",
    "#     torch.optim.Adam(new_params, lr=1e-3),\n",
    "#     10,\n",
    "#     best_on_disk=1e9,\n",
    "#     early_stop=100,\n",
    "#     path='new-models/',\n",
    "#     print_future_mse=True,\n",
    "#     hybrid_cde=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(new_vi,\n",
    "#             data_gen,\n",
    "#             64,\n",
    "#             5,\n",
    "#             mc_itr=50,\n",
    "#             real=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Further attempt (include expert dimension but don't use it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_gen.get_mini_batch(\"train\", 100)\n",
    "x = data[\"measurements\"]\n",
    "a = data[\"actions\"]\n",
    "mask = data[\"masks\"]\n",
    "z0 = data[\"latents\"]\n",
    "z0 = z0.numpy()\n",
    "print(z0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the numerical error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example differential equations\n",
    "def f(t, x):\n",
    "    return np.cos(t) - x\n",
    "\n",
    "def g(t, y):\n",
    "    # return np.cos(t) - y\n",
    "    return np.cos(t) - y\n",
    "\n",
    "# Integrating equation 1 over [0, 1] with time scaled by 14\n",
    "t1_span = (0, 1)\n",
    "t1_points = np.linspace(*t1_span, 100)\n",
    "solution1 = solve_ivp(lambda t, x: f(14*t, x), t1_span, [0], t_eval=t1_points)\n",
    "\n",
    "# Integrating equation 2 over [0, 14]\n",
    "t2_span = (0, 14)\n",
    "t2_points = np.linspace(*t2_span, 100)\n",
    "solution2 = solve_ivp(g, t2_span, [0], t_eval=t2_points)\n",
    "\n",
    "# Comparing solutions\n",
    "# Scaling t1_points by 14 to match the time points with t2_points for comparison\n",
    "scaled_t2_points = t2_points / 14\n",
    "\n",
    "# Plotting the solutions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(t1_points, solution1.y[0], label='Equation 1 (f(t, x))')\n",
    "plt.plot(scaled_t2_points, solution2.y[0]/14, label='Equation 2 (g(t/14, y))', linestyle='dashed')\n",
    "plt.title('Comparison of Differential Equations')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Solution')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
